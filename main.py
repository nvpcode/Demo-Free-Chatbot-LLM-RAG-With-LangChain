import streamlit as st
from dotenv import load_dotenv
import os
from langchain_community.callbacks.streamlit import StreamlitCallbackHandler
from langchain_community.chat_message_histories import StreamlitChatMessageHistory
from seed_data import load_data_from_folder, vector_store
from llm_gemini import func_retriever, get_llm

def setup_page():
    st.set_page_config(
        page_title="ChatBot_RAG",
        page_icon="ğŸ’¬",
        layout="wide"
    )

def initialize_app():
    load_dotenv()
    setup_page()

def setup_sidebar():
    with st.sidebar:
        st.title("âš™ï¸Cáº¥u hÃ¬nh")
        
        st.header("ğŸ“‘Embeddings Model")
        embeddings_choice = st.radio(
            "Chá»n Embeddings Model:",
            ["models/embedding-001"]
        )

        st.header("ğŸ§Model LLM")
        model_choice = st.radio(
            "Chá»n AI Model Ä‘á»ƒ tráº£ lá»i:",
            ["gemini-2.5-flash", "gemini-2.5-pro"]
        )

        st.header("ğŸ“Táº£i dá»¯ liá»‡u")
        uploaded_files = st.file_uploader(
            "Chá»n má»™t hoáº·c nhiá»u file dá»¯ liá»‡u .txt, docx, pdf",
            type=["txt", "docx", "pdf"],
            accept_multiple_files=True,
        )

        if uploaded_files:
            os.makedirs("data", exist_ok=True)
            for uploaded_file in uploaded_files:
                file_path = os.path.join("data", uploaded_file.name)
                with open(file_path, "wb") as f:
                    f.write(uploaded_file.read())
                st.success(f"âœ…ÄÃ£ lÆ°u file vÃ o {file_path}")

        return embeddings_choice, model_choice

def setup_chat_interface(model_choice):
    col1, col2 = st.columns([6, 1])
    with col1:
        st.title("ğŸ’¬Chat-NVP")
    with col2:
        if st.button("ğŸ”„ LÃ m má»›i há»™i thoáº¡i"):
            st.session_state.messages = [
                {"role": "assistant", "content": "Xin chÃ o! TÃ´i cÃ³ thá»ƒ giÃºp gÃ¬ cho báº¡n hÃ´m nay?"}
            ]
            st.rerun()

    st.caption(f"ğŸš€ Trá»£ lÃ½ AI Ä‘Æ°á»£c há»— trá»£ bá»Ÿi {model_choice}")

    msgs = StreamlitChatMessageHistory(key="langchain_messages")

    if "messages" not in st.session_state:
        st.session_state.messages = [
            {"role": "assistant", "content": "Xin chÃ o! TÃ´i cÃ³ thá»ƒ giÃºp gÃ¬ cho báº¡n hÃ´m nay?"}
        ]
        msgs.add_ai_message("Xin chÃ o! TÃ´i cÃ³ thá»ƒ giÃºp gÃ¬ cho báº¡n hÃ´m nay?")

    # ğŸ‘‰ Hiá»ƒn thá»‹ toÃ n bá»™ lá»‹ch sá»­ há»™i thoáº¡i
    for msg in st.session_state.messages:
        role = "assistant" if msg["role"] == "assistant" else "human"
        st.chat_message(role).write(msg["content"])

    return msgs

def process_user_message(msgs, retriever, model_choice):
    if prompt := st.chat_input("HÃ£y nháº­p cÃ¢u há»i cá»§a báº¡n..."):
        # Hiá»ƒn thá»‹ cÃ¢u há»i
        st.session_state.messages.append({"role": "human", "content": prompt})
        with st.chat_message("human"):
            st.markdown(prompt)
        msgs.add_user_message(prompt)

        # Táº¡o callback vÃ  láº¥y lá»‹ch sá»­
        with st.chat_message("assistant"):
            
            last_response = ""
            for msg in reversed(st.session_state.messages[:-1]):
                if msg["role"] == "assistant":
                    last_response = msg["content"]
                    break

            # GhÃ©p context tá»« cÃ¢u tráº£ lá»i gáº§n nháº¥t
            full_prompt = f"{last_response}\nCÃ¢u há»i: {prompt}" if last_response else prompt

            # Gá»i model  
            llm, final_prompt = get_llm(full_prompt, retriever, model_choice)
            st_callback = StreamlitCallbackHandler(st.container())
            response = llm.invoke(final_prompt, config={"callbacks": [st_callback]})
            answer = response.content if hasattr(response, "content") else str(response)
            st.markdown(answer)


        st.session_state.messages.append({"role": "assistant", "content": answer})
        msgs.add_ai_message(answer)

def main():
    initialize_app()
    embeddings_choice, model_choice = setup_sidebar()

    # Giao diá»‡n chat
    msgs = setup_chat_interface(model_choice)

    # Vector hÃ³a dá»¯ liá»‡u
    all_chunks = load_data_from_folder()

    if not all_chunks:
        st.warning("ğŸ“‚ChÆ°a cÃ³ dá»¯ liá»‡u Ä‘á»ƒ táº¡o embeddings. Vui lÃ²ng upload file .txt vÃ o sidebar.")
        return  # Dá»«ng láº¡i, khÃ´ng cháº¡y tiáº¿p main
    else:
        vector = vector_store(all_chunks, embeddings_choice)
        retriever = func_retriever(vector)

    # Chat
    process_user_message(msgs, retriever, model_choice)

if __name__ == "__main__":
    main()